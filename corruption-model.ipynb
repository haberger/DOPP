{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os \n",
    "import missingno as msno\n",
    "\n",
    "from data_cleanup import *\n",
    "from feature_selection import *\n",
    "from model_ import *\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from sklearn.metrics import mean_squared_error as rmse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, LassoLarsCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "Which country’s characteristics are good predictors of a country’s position on a corruption index?\n",
    "\n",
    "If we manage to find characteristics and predict country’s position on a corruption index reasonably well, are there countries whose characteristics don’t correspond to their position on a corruption index?\n",
    "\n",
    "If yes, does this hold across all corruption indexes, or do the lists of such countries vary from index to index?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "Our main dataset is the [QoG (Quality of Governence) Standard Time-Series Dataset](https://www.gu.se/en/quality-government/qog-data/data-downloads/standard-dataset) from the University of Gothenburg. It is a huge dataset with 2000 features that includes data from 1946 to 2021 for most countries. We chose it because we never really had to deal with dimensionality reduction yet and where interested in doing so. We additionally merge this dataset with the [ISO-3166-Countries with Regions](https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes/blob/master/all/all.csv) dataset to additionally obtain sub-region (f.e. Western Europe, South-East Asia, etc.) information, as we think the subregion of a country is very important for its corruption rankings. Due to this we are using the subregion to split the data in a stratified fashion into training/test-data.\n",
    "\n",
    "For region merging manual patching was required as some countries that where included in the QoG dataset where not inside the ISO-3166 dataset. Additionally we opted to merge the Micronesia/Melanesia/Polynesia subregions into a new, combined region called 'Pacific Island' as the original subregions only had a small amount of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "qog_dataset_filename = 'qog_std_ts_jan22.csv'\n",
    "df = pd.read_csv(join(data_dir, qog_dataset_filename), low_memory=False)\n",
    "\n",
    "df = merge_region(df)\n",
    "display(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some features with a very small amount of non-null values. We can also see that most features are numerical values or might be encoded as numerical values. Checking this with the documentation of the dataset shows that this indeed seems to be the case as all categorical variables were encoded numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True, memory_usage='deep', show_counts=True)\n",
    "df.describe()\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are looking at the correlation between corruption indices that we decided to investigate more closely after our preliminary research.\n",
    "We can see that most of them are highly correlated which makes sense as they generally try to estimate the same thing with slightly different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruption_col = ['bci_bci', 'ti_cpi', 'vdem_corr', 'vdem_execorr', 'vdem_jucorrdc', 'vdem_pubcorr', 'wbgi_cce', 'ti_cpi_om']\n",
    "corruption_corr = df[corruption_col].corr()\n",
    "mask = np.triu(np.ones_like(corruption_corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corruption_corr, mask=mask, cmap=cmap, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As numbers don't tell the whole story we also look at pairplots between the different corruption indices. The main corruption indices (bci_bci, ti_cpi and wbgi_cce) seem to be really close in their estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[corruption_col])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking at missing values next. We can see that previously less data was available. The closer we are to recent times the more data was available. We can also see a somewhat regular-ish pattern of missing values across multiple years. This most likely are countries for which no data was gathered across multiple years (f.e. Western countries generally started collecting extensive data earlier while countries in sub-saharan africa where rather late). \n",
    "\n",
    "Due to changes in the methodology for ti_cpi (which we can see quite nicely in the plots) we have to handle this case in a more special case. Explanations for it follow in the next steps.\n",
    "\n",
    "As we have 2000 features it doesn't really make sense to look at the distribution/value-ranges/missing values, ... etc for those, as this would take an unreasonable amount of time to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop every row where none of the corruption data is available\n",
    "df_any_corruption_info_available = df.dropna(subset=corruption_col, axis=0, how=\"all\")\n",
    "\n",
    "# drop every row where not every corruption data is available with special handling for ti_cpi\n",
    "df_cpi_combined = df.copy()\n",
    "df_cpi_combined['ti_cpi']=df['ti_cpi'].combine_first(df['ti_cpi_om'])\n",
    "df_all_corruption_info_available = df_cpi_combined.dropna(subset=corruption_col, axis=0, how='any')\n",
    "\n",
    "\n",
    "display(df_any_corruption_info_available.shape)\n",
    "#11k to 1.7k rows\n",
    "display(df_all_corruption_info_available.shape)\n",
    "\n",
    "corruption_col_with_year = ['year', 'bci_bci', 'ti_cpi', 'ti_cpi_om', 'vdem_corr', 'vdem_execorr', 'vdem_jucorrdc', 'vdem_pubcorr', 'wbgi_cce']\n",
    "ax = msno.matrix(df[corruption_col_with_year].sort_values(by='year'))\n",
    "ax.set_title('Missing values from whole dataframe sorted by year (most recent being lowest)', fontsize=18)\n",
    "ax = msno.matrix(df_any_corruption_info_available[corruption_col_with_year].sort_values(by='year'))\n",
    "ax.set_title('Missing values from dataframe with all rows where any corruption info is available sorted by year (most recent being lowest)', fontsize=18)\n",
    "# No surprises here\n",
    "corruption_col_with_year.remove('ti_cpi_om')\n",
    "ax = msno.matrix(df_all_corruption_info_available[corruption_col_with_year].sort_values(by='year'))\n",
    "ax.set_title('Missing values from dataframe with all rows where all corruption info is available sorted by year (most recent being lowest)', fontsize=18)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its also important to get an idea of the value range for each corruption metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[corruption_col].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we are dropping some features that were directly used by the corruption indices that we want to predict. If we would keep them our feature selection just would end up picking those features and we would only learn the exact same model that the institutions used to calculate their corruption indices. We opted for a smaller amount of corruption indices so that the scope of this exercise didn't get to big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with corruption indices\n",
    "corr_cols = ['ti_cpi', 'bci_bci', 'ti_cpi_om', 'wbgi_cce'] \n",
    "# columns with metadata\n",
    "meta_cols = ['ccode', 'ccode_qog', 'ccodealp', 'ccodealp_year', 'ccodecow', 'cname', 'cname_qog', 'cname_year', 'version', 'year', 'region', 'sub-region']\n",
    "df_reduced = drop_certain_columns(df, corr_cols, meta_cols)\n",
    "display(df_reduced.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains data from 1960-2021. As most corruption indices only started to be available in the 1990s the data from the years before that are not that interesting(we only consider 'this' years data to calculate 'this' years result). Additionally we will also end up training one model for each corruption index and as we want to make comparisons between them we will have to use the exact same train and testset for all of them. Due to this we only keep the rows where values for all corruption indices are available. Unfortunately ti_cpi (Transparency international - Corruption Perception Index) drastically changed its methodology in 2012, which means that we have two variables for it ti_cpi and ti_cpi_om (old methodology), as they can't be directly compared. If we would restrict the data to only include the new methodology we would have a very small dataset. But the old methodology is not as interesting as it is a relative measure (relative to other countries in that year) that can't be directly compared across different years (so having the same ti_cpi_om-value in 1990 and 1995 could mean different things). \n",
    "\n",
    "Because of this we opted to keep rows where the union of ti_cpi and ti_cpi_om with the caveat that \n",
    "- those models can't be directly compared to the other models (bci_bci and wbgi_cce) because they don't have the same training/testdata\n",
    "- the model for ti_cpi_om might not be that meaningful due to being a relative measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = return_rows_where_all_corruption_data_is_available(df_reduced, corr_cols)\n",
    "display(df_reduced.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we drop all rows that have less than 10% non-nan values as they don't hold that much information anyway and because we are still left with a very large number of features.\n",
    "Then we transform features that have less than 10 unique values in 2836 rows into categorical variables. This picks up a small amount of false positives but in a huge majority of cases this transformation was correct. Due to the high amount of features we didn't bother with doing it 'correctly' (which would mean by hand).\n",
    "\n",
    "As we still have a very high amount of features we drop all columns that have any nan value so that we don't have to correctly handle them. We are left with 700 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = drop_columns_with_nan_values(df_reduced)\n",
    "display(df_reduced.shape)\n",
    "df_reduced = transform_to_categorical(df_reduced)\n",
    "display(df_reduced.shape)\n",
    "\n",
    "df_reduced.loc[:,['ti_cpi', 'ti_cpi_om']] = df_reduced.loc[:,['ti_cpi', 'ti_cpi_om']].replace(np.NaN, -5)\n",
    "df_reduced = df_reduced.dropna(how='any', axis=1).copy()\n",
    "df_reduced.loc[:,['ti_cpi', 'ti_cpi_om']] = df_reduced.loc[:,['ti_cpi', 'ti_cpi_om']].replace(-5, np.NaN)\n",
    "display(df_reduced.shape)\n",
    "df = df_reduced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_date_columns(df)\n",
    "\n",
    "best_features_dict = {}\n",
    "selected_features_dict = {}\n",
    "\n",
    "for target_col in corr_cols:\n",
    "    X_train, X_test, y_train, y_test = create_traintestsplit(df, corr_cols = corr_cols, meta_cols=meta_cols, target_col=target_col)\n",
    "    \n",
    "    best_features = pre_select(X_train, y_train)\n",
    "    best_features_dict[target_col] = set(best_features)\n",
    "    df_train = X_train[best_features].copy()\n",
    "    df_train[target_col]=y_train\n",
    "    mce = MultiCollinearityEliminator(df_train, target_col, 0.85)\n",
    "    feaures_no_collinearity = list(mce.autoEliminateMulticollinearity().columns)\n",
    "    feaures_no_collinearity.remove(target_col)\n",
    "    selected_features_dict[target_col] = set(feaures_no_collinearity)\n",
    "\n",
    "\n",
    "#selected_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_union=list(set.union(*list(best_features_dict.values())))\n",
    "best_features_intersection=list(set.intersection(*list(best_features_dict.values())))\n",
    "\n",
    "best_features_intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_union=list(set.union(*list(selected_features_dict.values())))\n",
    "selected_features_intersection=list(set.intersection(*list(selected_features_dict.values())))\n",
    "\n",
    "selected_features_intersection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Try Lasso and Random Forest next. Train models for different feature configurations \n",
    "\n",
    "    - individual selected features for a particular index\n",
    "    - union of all good features for all indices\n",
    "    - intersection of all selected for features for all indices\n",
    "\n",
    "As scores r2 and rmse are reported. The comparisons are based on r2-scores as they make the scores for different indices comparable.\n",
    "    \n",
    "\n",
    "### Lasso\n",
    "The used library uses cross validation to determine a good value for alpha.\n",
    "\n",
    "The following script trains for all target indices a Lasso model, then displays r2 score and feature importance information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_info_script(features, name):\n",
    "    lasso_bf = dict()\n",
    "\n",
    "    df_score = pd.DataFrame(columns=['r2', 'rmse'], index=corr_cols)\n",
    "    for target in corr_cols:\n",
    "        if isinstance(features, dict):\n",
    "            lasso_bf[target] = apply_lassocv(df, target, list(features[target]), corr_cols, meta_cols, fprint=False)\n",
    "        else:\n",
    "            lasso_bf[target] = apply_lassocv(df, target, features, corr_cols, meta_cols, fprint=False)\n",
    "        df_score.loc[target,] = [lasso_bf[target]['r2'] ,lasso_bf[target]['rmse']]\n",
    "    \n",
    "    print('scores')\n",
    "    display(df_score)\n",
    "\n",
    "    # l_fi = [lasso_bf[target]['feat_importance'] for target in corr_cols]\n",
    "    # df_fi = pd.concat(l_fi)\n",
    "\n",
    "    # l_firk = [lasso_bf[target]['feat_importance_rank'] for target in corr_cols]\n",
    "    # df_firk = pd.concat(l_firk)\n",
    "\n",
    "    # print('feature importance')\n",
    "    # display(df_fi)\n",
    "    # df_fi.T.plot(kind='bar', figsize=(20,8))\n",
    "    # print()\n",
    "\n",
    "    # print('feature importance rank')\n",
    "    # display(df_firk)\n",
    "    # print()\n",
    "    # file = os.path.join('pickle', name +'.obj')\n",
    "    # f = open(file, 'wb')\n",
    "    # pickle.dump(lasso_bf ,f)\n",
    "    #f.close()     \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we apply the script for the individually selected features for each corruption index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_info_script(selected_features_dict, 'lasso_selected_features_dict')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use for all corruption indices the same set of features - the set of all as promising declared features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_info_script(best_features_union, 'lasso_best_features_union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes[best_features_union]\n",
    "df.br_mon.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use only the features that are in all individually selected feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_info_script(selected_features_intersection, 'lasso_selected_features_intersection')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "\n",
    "- wbgi_rle (Rule of Law) is by far the most important feature in almost all configurations\n",
    "- most indices behave similarly for the three feature set configuration but\n",
    "- ti_cpi is most different: its score is very bad with the smallest feature set. Its most important feature is wbgi_pvs [Political Stability and Absence of Violence/Terrorism, Standard error] and not wbgi_rle\n",
    "- vdem_jucorrdc is also effected more by different feature sets and its score is lower as well in general.\n",
    "- all the other indices gain information slightly by more features but they do not rely too much on the chosen setups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Next we do the same for a Random Forest Regressor. Here initially no cross validation is done. We just use a default setup at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_info_script(features, name):\n",
    "    rf_bf = dict()\n",
    "\n",
    "    df_score = pd.DataFrame(columns=['r2', 'rmse'], index=corr_cols)\n",
    "    for target in corr_cols:\n",
    "        if isinstance(features, dict):\n",
    "            rf_bf[target] = apply_rf(df, target, list(features[target]), corr_cols, meta_cols, fprint=False)\n",
    "        else:\n",
    "            rf_bf[target] = apply_rf(df, target, features, corr_cols, meta_cols, fprint=False)\n",
    "        df_score.loc[target,] = [rf_bf[target]['r2'] ,rf_bf[target]['rmse']]\n",
    "    \n",
    "    print('scores')\n",
    "    display(df_score)\n",
    "\n",
    "    # l_fi = [rf_bf[target]['feat_importance'] for target in corr_cols]\n",
    "    # df_fi = pd.concat(l_fi)\n",
    "\n",
    "    # l_firk = [rf_bf[target]['feat_importance_rank'] for target in corr_cols]\n",
    "    # df_firk = pd.concat(l_firk)\n",
    "\n",
    "    # print('feature importance')\n",
    "    # display(df_fi)\n",
    "\n",
    "    # df_fi.T.plot(kind='bar', figsize=(20,8))\n",
    "    # print()\n",
    "\n",
    "    # print('feature importance rank')\n",
    "    # display(df_firk)\n",
    "    # print()\n",
    "    # file = os.path.join('pickle', name +'.obj')\n",
    "    # f = open(file, 'wb')\n",
    "    # pickle.dump(rf_bf ,f)\n",
    "    #f.close()    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we apply again the script for the individually selected features for each corruption index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_info_script(selected_features_dict, 'rf_selected_features_dict')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use for all corruption indices the same set of features - the set of all as promising declared features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_info_script(best_features_union, 'rf_best_features_union')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use only the features that are in all individually selected feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_info_script(selected_features_intersection, 'rf_best_features_union')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general picture of the results with Random Forest is not that different to the one with Lasso. Some differences are\n",
    "\n",
    "- ti_cpi is predicted very well now both in comparison with Lasso and with all other indices\n",
    "- HOWEVER, if only the minimal feature set is used ti_cpi is even worse than with Lasso\n",
    "- for vdem_execorr the vdem_egal (Egalitarian component index) is the most important feature\n",
    "- vdem_jucorr is now by far the most difficult to predict index\n",
    "- although feature importance is not straight-forward comparable between Lasso (weight of coefficients) and Random Forest (Gini) it seems like Random Forst discriminates harder with regard to features\n",
    "\n",
    "Random Forest performs either similarly or better for most setups / indices allthough no parameter optimization is done by now. So we continue with Random Forst and do hyperparameter optimization for some specific settings next to further optimize the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search: Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cross validation / hyperparameter grid search better parameters are determined. With those optimizations then again models are trained, then the test set is predicted and scores are evaluated.\n",
    "\n",
    "The script defined below shows a similar report than above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_gridsearch_info_script(features, name):\n",
    "    rf_bf = dict()\n",
    "\n",
    "    param_grid = {\n",
    "        \"randomforestregressor__max_depth\": [2, 3, 5, 10, None],\n",
    "        \"randomforestregressor__min_samples_split\": [2, 3, 5, 10],\n",
    "        \"randomforestregressor__max_features\": [\"log2\", None]\n",
    "        }\n",
    "\n",
    "    df_score = pd.DataFrame(columns=['r2', 'rmse'], index=corr_cols)\n",
    "    for target in corr_cols:\n",
    "        if isinstance(features, dict):\n",
    "            rf_bf[target] = apply_gridsearch_rf(df, target, list(features[target]), param_grid, corr_cols, meta_cols, fprint=False)\n",
    "        else:\n",
    "            rf_bf[target] = apply_gridsearch_rf(df, target, features, param_grid, corr_cols, meta_cols, fprint=False)\n",
    "        df_score.loc[target,] = [rf_bf[target]['r2'] ,rf_bf[target]['rmse']]\n",
    "    \n",
    "    print('scores')\n",
    "    display(df_score)\n",
    "\n",
    "    # l_fi = [rf_bf[target]['feat_importance'] for target in corr_cols]\n",
    "    # df_fi = pd.concat(l_fi)\n",
    "    # rf_bf[target]\n",
    "    # l_firk = [rf_bf[target]['feat_importance_rank'] for target in corr_cols]\n",
    "    # df_firk = pd.concat(l_firk)\n",
    "    # rf_bf[target]['params']\n",
    "    # l_params = [rf_bf[target]['params'] for target in corr_cols]\n",
    "    # df_params = pd.concat(l_params)\n",
    "\n",
    "    # print('feature importance')\n",
    "    # display(df_fi)\n",
    "\n",
    "    # df_fi.T.plot(kind='bar', figsize=(20,8))\n",
    "    # print()\n",
    "\n",
    "    # print('feature importance rank')\n",
    "    # display(df_firk)\n",
    "    \n",
    "    # file = os.path.join('pickle', name +'.obj')\n",
    "    # f = open(file, 'wb')\n",
    "    # pickle.dump(rf_bf ,f)\n",
    "    #f.close()    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only use for each index the individually selected feature set as we saw above that the results are comparable (so the feature selection process works adequately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gridsearch_info_script(selected_features_dict, 'rf_grid_selected_features_dict')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most indices the hyperparameter optimization does not seem to significantly improve the r2-scores. But for vdem_jucorrdc it seems to improve. For vdem_pubcorr and wbgi_cce the improvement is minor.\n",
    "\n",
    "The feature importance (figure) changes a lot more. Here we see for all but bci_bci that relatively wbgi_rle is not as important anymore. This is most likely due to the max_samples_features being log2 now. One could argue if the original model where wbgi_rle is the main feature is simpler and from the same quality or on the other side that other features are also able to replace wbgi_rle when combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bf = rf_gridsearch_info_script(selected_features_union, 'rf_grid_selected_features_union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open('pickle/rf_grid_selected_features_union.obj', 'rb')\n",
    "rf_bf = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "l_fi = [rf_bf[target]['feat_importance'] for target in corr_cols]\n",
    "df_fi = pd.concat(l_fi)\n",
    "l_firk = [rf_bf[target]['feat_importance_rank'] for target in corr_cols]\n",
    "df_firk = pd.concat(l_firk)\n",
    "l_params = [rf_bf[target]['params'] for target in corr_cols]\n",
    "df_params = pd.concat(l_params)\n",
    "\n",
    "print('feature importance')\n",
    "display(df_fi)\n",
    "\n",
    "\n",
    "df_sorted = df_fi.reindex(df_fi.mean().sort_values().index[::-1], axis=1)\n",
    "\n",
    "display(df_sorted)\n",
    "df_sorted.T.plot(kind='bar', figsize=(20,8))\n",
    "\n",
    "df_fi.T.plot(kind='bar', figsize=(20,8))\n",
    "print()\n",
    "\n",
    "print('feature importance rank')\n",
    "display(df_firk)\n",
    "print()\n",
    "print(df_sorted.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_fi = df_fi.reindex(df_fi.mean().sort_values(ascending=False).index, axis=1)\n",
    "col_names = df_fi.columns\n",
    "df_fi = df_fi.T.melt(\n",
    "    ignore_index=False,\n",
    "    value_vars = ['ti_cpi', 'bci_bci', 'ti_cpi_om', 'wbgi_cce'],\n",
    "    value_name = 'feature_importance'\n",
    ").reset_index().rename(columns={'index': 'feature', 'variable': 'corruption_index'})\n",
    "\n",
    "plt.rcdefaults()\n",
    "font = {'family' : 'normal',\n",
    "    'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(df_fi, x='feature',  y='feature_importance', hue='corruption_index', palette='magma', width=0.6)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dopp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:36:39) [GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e2fc646e30e64531842a4bfe05e2da34f8d661157b760331bcf2bd7afbffd2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
