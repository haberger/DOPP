{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os \n",
    "\n",
    "from data_cleanup import *\n",
    "from feature_selection import *\n",
    "from model_ import *\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from sklearn.metrics import mean_squared_error as rmse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, LassoLarsCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = ['ti_cpi', 'bci_bci', 'ti_cpi_om', 'wbgi_cce']\n",
    "meta_cols = ['ccode', 'ccode_qog', 'ccodealp', 'ccodealp_year', 'ccodecow', 'cname', 'cname_qog', 'cname_year', 'version', 'year', 'region', 'sub-region']\n",
    "df = load_reduced_df(corr_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_date_columns(df)\n",
    "\n",
    "best_features_dict = {}\n",
    "selected_features_dict = {}\n",
    "\n",
    "for target_col in corr_cols:\n",
    "    X_train, X_test, y_train, y_test = create_traintestsplit(df, corr_cols = corr_cols, meta_cols=meta_cols, target_col=target_col)\n",
    "    \n",
    "    best_features = pre_select(X_train, y_train)\n",
    "    best_features_dict[target_col] = set(best_features)\n",
    "    df_train = X_train[best_features].copy()\n",
    "    df_train[target_col]=y_train\n",
    "    mce = MultiCollinearityEliminator(df_train, target_col, 0.85)\n",
    "    feaures_no_collinearity = list(mce.autoEliminateMulticollinearity().columns)\n",
    "    feaures_no_collinearity.remove(target_col)\n",
    "    selected_features_dict[target_col] = set(feaures_no_collinearity)\n",
    "\n",
    "\n",
    "#selected_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_union=list(set.union(*list(best_features_dict.values())))\n",
    "best_features_intersection=list(set.intersection(*list(best_features_dict.values())))\n",
    "\n",
    "best_features_intersection\n",
    "\n",
    "print(df[best_features_union].isna().sum().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_union=list(set.union(*list(selected_features_dict.values())))\n",
    "selected_features_intersection=list(set.intersection(*list(selected_features_dict.values())))\n",
    "\n",
    "selected_features_intersection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Try Lasso and Random Forest next. Train models for different feature configurations \n",
    "\n",
    "    - individual selected features for a particular index\n",
    "    - union of all good features for all indices\n",
    "    - intersection of all selected for features for all indices\n",
    "\n",
    "As scores r2 and rmse are reported. The comparisons are based on r2-scores as they make the scores for different indices comparable.\n",
    "    \n",
    "\n",
    "### Lasso\n",
    "The used library uses cross validation to determine a good value for alpha.\n",
    "\n",
    "The following script trains for all target indices a Lasso model, then displays r2 score and feature importance information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_info_script(features, name):\n",
    "    lasso_bf = dict()\n",
    "\n",
    "    df_score = pd.DataFrame(columns=['r2', 'rmse'], index=corr_cols)\n",
    "    for target in corr_cols:\n",
    "        if isinstance(features, dict):\n",
    "            lasso_bf[target] = apply_lassocv(df, target, list(features[target]), corr_cols, meta_cols, fprint=False)\n",
    "        else:\n",
    "            lasso_bf[target] = apply_lassocv(df, target, features, corr_cols, meta_cols, fprint=False)\n",
    "        df_score.loc[target,] = [lasso_bf[target]['r2'] ,lasso_bf[target]['rmse']]\n",
    "    \n",
    "    print('scores')\n",
    "    display(df_score)\n",
    "\n",
    "    # l_fi = [lasso_bf[target]['feat_importance'] for target in corr_cols]\n",
    "    # df_fi = pd.concat(l_fi)\n",
    "\n",
    "    # l_firk = [lasso_bf[target]['feat_importance_rank'] for target in corr_cols]\n",
    "    # df_firk = pd.concat(l_firk)\n",
    "\n",
    "    # print('feature importance')\n",
    "    # display(df_fi)\n",
    "    # df_fi.T.plot(kind='bar', figsize=(20,8))\n",
    "    # print()\n",
    "\n",
    "    # print('feature importance rank')\n",
    "    # display(df_firk)\n",
    "    # print()\n",
    "    # file = os.path.join('pickle', name +'.obj')\n",
    "    # f = open(file, 'wb')\n",
    "    # pickle.dump(lasso_bf ,f)\n",
    "    #f.close()     \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we apply the script for the individually selected features for each corruption index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_info_script(selected_features_dict, 'lasso_selected_features_dict')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use for all corruption indices the same set of features - the set of all as promising declared features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_info_script(best_features_union, 'lasso_best_features_union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes[best_features_union]\n",
    "df.br_mon.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use only the features that are in all individually selected feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_info_script(selected_features_intersection, 'lasso_selected_features_intersection')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "\n",
    "- wbgi_rle (Rule of Law) is by far the most important feature in almost all configurations\n",
    "- most indices behave similarly for the three feature set configuration but\n",
    "- ti_cpi is most different: its score is very bad with the smallest feature set. Its most important feature is wbgi_pvs [Political Stability and Absence of Violence/Terrorism, Standard error] and not wbgi_rle\n",
    "- vdem_jucorrdc is also effected more by different feature sets and its score is lower as well in general.\n",
    "- all the other indices gain information slightly by more features but they do not rely too much on the chosen setups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Next we do the same for a Random Forest Regressor. Here initially no cross validation is done. We just use a default setup at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_info_script(features, name):\n",
    "    rf_bf = dict()\n",
    "\n",
    "    df_score = pd.DataFrame(columns=['r2', 'rmse'], index=corr_cols)\n",
    "    for target in corr_cols:\n",
    "        if isinstance(features, dict):\n",
    "            rf_bf[target] = apply_rf(df, target, list(features[target]), corr_cols, meta_cols, fprint=False)\n",
    "        else:\n",
    "            rf_bf[target] = apply_rf(df, target, features, corr_cols, meta_cols, fprint=False)\n",
    "        df_score.loc[target,] = [rf_bf[target]['r2'] ,rf_bf[target]['rmse']]\n",
    "    \n",
    "    print('scores')\n",
    "    display(df_score)\n",
    "\n",
    "    # l_fi = [rf_bf[target]['feat_importance'] for target in corr_cols]\n",
    "    # df_fi = pd.concat(l_fi)\n",
    "\n",
    "    # l_firk = [rf_bf[target]['feat_importance_rank'] for target in corr_cols]\n",
    "    # df_firk = pd.concat(l_firk)\n",
    "\n",
    "    # print('feature importance')\n",
    "    # display(df_fi)\n",
    "\n",
    "    # df_fi.T.plot(kind='bar', figsize=(20,8))\n",
    "    # print()\n",
    "\n",
    "    # print('feature importance rank')\n",
    "    # display(df_firk)\n",
    "    # print()\n",
    "    # file = os.path.join('pickle', name +'.obj')\n",
    "    # f = open(file, 'wb')\n",
    "    # pickle.dump(rf_bf ,f)\n",
    "    #f.close()    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we apply again the script for the individually selected features for each corruption index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_info_script(selected_features_dict, 'rf_selected_features_dict')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use for all corruption indices the same set of features - the set of all as promising declared features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_info_script(best_features_union, 'rf_best_features_union')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use only the features that are in all individually selected feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_info_script(selected_features_intersection, 'rf_best_features_union')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general picture of the results with Random Forest is not that different to the one with Lasso. Some differences are\n",
    "\n",
    "- ti_cpi is predicted very well now both in comparison with Lasso and with all other indices\n",
    "- HOWEVER, if only the minimal feature set is used ti_cpi is even worse than with Lasso\n",
    "- for vdem_execorr the vdem_egal (Egalitarian component index) is the most important feature\n",
    "- vdem_jucorr is now by far the most difficult to predict index\n",
    "- although feature importance is not straight-forward comparable between Lasso (weight of coefficients) and Random Forest (Gini) it seems like Random Forst discriminates harder with regard to features\n",
    "\n",
    "Random Forest performs either similarly or better for most setups / indices allthough no parameter optimization is done by now. So we continue with Random Forst and do hyperparameter optimization for some specific settings next to further optimize the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search: Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cross validation / hyperparameter grid search better parameters are determined. With those optimizations then again models are trained, then the test set is predicted and scores are evaluated.\n",
    "\n",
    "The script defined below shows a similar report than above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_gridsearch_info_script(features, name):\n",
    "    rf_bf = dict()\n",
    "\n",
    "    param_grid = {\n",
    "        \"randomforestregressor__max_depth\": [2, 3, 5, 10, None],\n",
    "        \"randomforestregressor__min_samples_split\": [2, 3, 5, 10],\n",
    "        \"randomforestregressor__max_features\": [\"log2\", None]\n",
    "        }\n",
    "\n",
    "    df_score = pd.DataFrame(columns=['r2', 'rmse'], index=corr_cols)\n",
    "    for target in corr_cols:\n",
    "        if isinstance(features, dict):\n",
    "            rf_bf[target] = apply_gridsearch_rf(df, target, list(features[target]), param_grid, corr_cols, meta_cols, fprint=False)\n",
    "        else:\n",
    "            rf_bf[target] = apply_gridsearch_rf(df, target, features, param_grid, corr_cols, meta_cols, fprint=False)\n",
    "        df_score.loc[target,] = [rf_bf[target]['r2'] ,rf_bf[target]['rmse']]\n",
    "    \n",
    "    print('scores')\n",
    "    display(df_score)\n",
    "\n",
    "    # l_fi = [rf_bf[target]['feat_importance'] for target in corr_cols]\n",
    "    # df_fi = pd.concat(l_fi)\n",
    "    # rf_bf[target]\n",
    "    # l_firk = [rf_bf[target]['feat_importance_rank'] for target in corr_cols]\n",
    "    # df_firk = pd.concat(l_firk)\n",
    "    # rf_bf[target]['params']\n",
    "    # l_params = [rf_bf[target]['params'] for target in corr_cols]\n",
    "    # df_params = pd.concat(l_params)\n",
    "\n",
    "    # print('feature importance')\n",
    "    # display(df_fi)\n",
    "\n",
    "    # df_fi.T.plot(kind='bar', figsize=(20,8))\n",
    "    # print()\n",
    "\n",
    "    # print('feature importance rank')\n",
    "    # display(df_firk)\n",
    "    \n",
    "    # file = os.path.join('pickle', name +'.obj')\n",
    "    # f = open(file, 'wb')\n",
    "    # pickle.dump(rf_bf ,f)\n",
    "    #f.close()    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we only use for each index the individually selected feature set as we saw above that the results are comparable (so the feature selection process works adequately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gridsearch_info_script(selected_features_dict, 'rf_grid_selected_features_dict')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most indices the hyperparameter optimization does not seem to significantly improve the r2-scores. But for vdem_jucorrdc it seems to improve. For vdem_pubcorr and wbgi_cce the improvement is minor.\n",
    "\n",
    "The feature importance (figure) changes a lot more. Here we see for all but bci_bci that relatively wbgi_rle is not as important anymore. This is most likely due to the max_samples_features being log2 now. One could argue if the original model where wbgi_rle is the main feature is simpler and from the same quality or on the other side that other features are also able to replace wbgi_rle when combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bf = rf_gridsearch_info_script(selected_features_union, 'rf_grid_selected_features_union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open('pickle/rf_grid_selected_features_union.obj', 'rb')\n",
    "rf_bf = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "l_fi = [rf_bf[target]['feat_importance'] for target in corr_cols]\n",
    "df_fi = pd.concat(l_fi)\n",
    "l_firk = [rf_bf[target]['feat_importance_rank'] for target in corr_cols]\n",
    "df_firk = pd.concat(l_firk)\n",
    "l_params = [rf_bf[target]['params'] for target in corr_cols]\n",
    "df_params = pd.concat(l_params)\n",
    "\n",
    "print('feature importance')\n",
    "display(df_fi)\n",
    "\n",
    "\n",
    "df_sorted = df_fi.reindex(df_fi.mean().sort_values().index[::-1], axis=1)\n",
    "\n",
    "display(df_sorted)\n",
    "df_sorted.T.plot(kind='bar', figsize=(20,8))\n",
    "\n",
    "df_fi.T.plot(kind='bar', figsize=(20,8))\n",
    "print()\n",
    "\n",
    "print('feature importance rank')\n",
    "display(df_firk)\n",
    "print()\n",
    "print(df_sorted.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_fi = df_fi.reindex(df_fi.mean().sort_values(ascending=False).index, axis=1)\n",
    "col_names = df_fi.columns\n",
    "df_fi = df_fi.T.melt(\n",
    "    ignore_index=False,\n",
    "    value_vars = ['ti_cpi', 'bci_bci', 'ti_cpi_om', 'wbgi_cce'],\n",
    "    value_name = 'feature_importance'\n",
    ").reset_index().rename(columns={'index': 'feature', 'variable': 'corruption_index'})\n",
    "\n",
    "plt.rcdefaults()\n",
    "font = {'family' : 'normal',\n",
    "    'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(df_fi, x='feature',  y='feature_importance', hue='corruption_index', palette='magma', width=0.6)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dopp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "543aa23b88fc9de87e855576d5d47491130665cd749e56028ebe7cfa2e718f2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
