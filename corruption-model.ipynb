{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os \n",
    "import missingno as msno\n",
    "from plotly import express as px\n",
    "\n",
    "from data_cleanup import *\n",
    "from feature_selection import *\n",
    "from model_ import *\n",
    "from country_evaluation import *\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from sklearn.metrics import mean_squared_error as rmse\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, LassoLarsCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "Which country’s characteristics are good predictors of a country’s position on a corruption index?\n",
    "\n",
    "If we manage to find characteristics and predict country’s position on a corruption index reasonably well, are there countries whose characteristics don’t correspond to their position on a corruption index?\n",
    "\n",
    "If yes, does this hold across all corruption indexes, or do the lists of such countries vary from index to index?\n",
    "\n",
    "We chose these questions because corruption indices are very rough estimates based on perception and trying to replicate the data with country characteristics seemed interesting and worth doing. Corruption indices often get criticized about potential biases so we also wanted to check if we would find some patterns regarding outliers in our data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "Our main dataset is the [QoG (Quality of Governence) Standard Time-Series Dataset](https://www.gu.se/en/quality-government/qog-data/data-downloads/standard-dataset) from the University of Gothenburg. It is a huge dataset about the quality of governance with 2000 features that includes data from 1946 to 2021 for most countries. We chose it because we never really had to deal with dimensionality reduction yet and where interested in doing so. We additionally merge this dataset with the [ISO-3166-Countries with Regions](https://github.com/lukes/ISO-3166-Countries-with-Regional-Codes/blob/master/all/all.csv) dataset to additionally obtain sub-region (f.e. Western Europe, South-East Asia, etc.) information, as we think the subregion of a country is very important for its corruption rankings. Due to this we are using the subregion to split the data in a stratified fashion into training/test-data.\n",
    "\n",
    "For region merging manual patching was required as some countries that where included in the QoG dataset where not inside the ISO-3166 dataset. Additionally we opted to merge the Micronesia/Melanesia/Polynesia subregions into a new, combined region called 'Pacific Island' as the original subregions only had a small amount of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "qog_dataset_filename = 'qog_std_ts_jan22.csv'\n",
    "df = pd.read_csv(join(data_dir, qog_dataset_filename), low_memory=False)\n",
    "\n",
    "df = merge_region(df)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are some features with a very small amount of non-null values. We can also see that most features are numerical values or might be encoded as numerical values. Checking this with the documentation of the dataset shows that this indeed seems to be the case as all categorical variables were encoded numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True, memory_usage='deep', show_counts=True)\n",
    "df.describe()\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are looking at the correlation between corruption indices that we decided to investigate more closely after our preliminary research.\n",
    "We can see that most of them are highly correlated which makes sense as they generally try to estimate the same thing with slightly different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruption_col = ['bci_bci', 'ti_cpi', 'vdem_corr', 'vdem_execorr', 'vdem_jucorrdc', 'vdem_pubcorr', 'wbgi_cce', 'ti_cpi_om']\n",
    "corruption_corr = df[corruption_col].corr()\n",
    "mask = np.triu(np.ones_like(corruption_corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "sns.heatmap(corruption_corr, mask=mask, cmap=cmap, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As numbers don't tell the whole story we also look at pairplots between the different corruption indices. The main corruption indices (bci_bci, ti_cpi and wbgi_cce) seem to be really close in their estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[corruption_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking at missing values next. We can see that previously less data was available. The closer we are to recent times the more data was available. We can also see a somewhat regular-ish pattern of missing values across multiple years. This most likely are countries for which no data was gathered across multiple years (f.e. Western countries generally started collecting extensive data earlier while countries in sub-saharan africa where rather late). \n",
    "\n",
    "Due to changes in the methodology for ti_cpi (which we can see quite nicely in the plots) we have to handle this case in a more special case. Explanations for it follow in the next steps.\n",
    "\n",
    "As we have 2000 features it doesn't really make sense to look at the distribution/value-ranges/missing values, ... etc for those, as this would take an unreasonable amount of time to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop every row where none of the corruption data is available\n",
    "df_any_corruption_info_available = df.dropna(subset=corruption_col, axis=0, how=\"all\")\n",
    "\n",
    "# drop every row where not every corruption data is available with special handling for ti_cpi\n",
    "df_cpi_combined = df.copy()\n",
    "df_cpi_combined['ti_cpi']=df['ti_cpi'].combine_first(df['ti_cpi_om'])\n",
    "df_all_corruption_info_available = df_cpi_combined.dropna(subset=corruption_col, axis=0, how='any')\n",
    "\n",
    "\n",
    "display(df_any_corruption_info_available.shape)\n",
    "#11k to 1.7k rows\n",
    "display(df_all_corruption_info_available.shape)\n",
    "\n",
    "corruption_col_with_year = ['year', 'bci_bci', 'ti_cpi', 'ti_cpi_om', 'vdem_corr', 'vdem_execorr', 'vdem_jucorrdc', 'vdem_pubcorr', 'wbgi_cce']\n",
    "ax = msno.matrix(df[corruption_col_with_year].sort_values(by='year'))\n",
    "ax.set_title('Missing values from whole dataframe sorted by year (most recent being lowest)', fontsize=18)\n",
    "ax = msno.matrix(df_any_corruption_info_available[corruption_col_with_year].sort_values(by='year'))\n",
    "ax.set_title('Missing values from dataframe with all rows where any corruption info is available sorted by year (most recent being lowest)', fontsize=18)\n",
    "\n",
    "corruption_col_with_year.remove('ti_cpi_om')\n",
    "# No surprises here\n",
    "ax = msno.matrix(df_all_corruption_info_available[corruption_col_with_year].sort_values(by='year'))\n",
    "ax.set_title('Missing values from dataframe with all rows where all corruption info is available sorted by year (most recent being lowest)', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its also important to get an idea of the value range for each corruption metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[corruption_col].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we are dropping some features that were directly used by the corruption indices that we want to predict. If we would keep them our feature selection just would end up picking those features and we would only learn the exact same model that the institutions used to calculate their corruption indices. We opted for a smaller amount of corruption indices so that the scope of this exercise didn't get to big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns with corruption indices\n",
    "corr_cols = ['ti_cpi', 'bci_bci', 'ti_cpi_om', 'wbgi_cce'] \n",
    "# columns with metadata\n",
    "meta_cols = ['ccode', 'ccode_qog', 'ccodealp', 'ccodealp_year', 'ccodecow', 'cname', 'cname_qog', 'cname_year', 'version', 'year', 'region', 'sub-region']\n",
    "df_reduced = drop_certain_columns(df, corr_cols, meta_cols)\n",
    "display(df_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains data from 1960-2021. As most corruption indices only started to be available in the 1990s the data from the years before that are not that interesting(we only consider 'this' years data to calculate 'this' years result). Additionally we will also end up training one model for each corruption index and as we want to make comparisons between them we will have to use the exact same train and testset for all of them. Due to this we only keep the rows where values for all corruption indices are available. Unfortunately ti_cpi (Transparency international - Corruption Perception Index) drastically changed its methodology in 2012, which means that we have two variables for it ti_cpi and ti_cpi_om (old methodology), as they can't be directly compared. If we would restrict the data to only include the new methodology we would have a very small dataset. But the old methodology is not as interesting as it is a relative measure (relative to other countries in that year) that can't be directly compared across different years (so having the same ti_cpi_om-value in 1990 and 1995 could mean different things). \n",
    "\n",
    "Because of this we opted to keep rows where the union of ti_cpi and ti_cpi_om with the caveat that \n",
    "- those models can't be directly compared to the other models (bci_bci and wbgi_cce) because they don't have the same training/testdata\n",
    "- the model for ti_cpi_om might not be that meaningful due to being a relative measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = return_rows_where_all_corruption_data_is_available(df_reduced, corr_cols)\n",
    "display(df_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we drop all rows that have less than 10% non-nan values as they don't hold that much information anyway and because we are still left with a very large number of features.\n",
    "Then we transform features that have less than 10 unique values in 2836 rows into categorical variables. This picks up a small amount of false positives but in a huge majority of cases this transformation was correct. Due to the high amount of features we didn't bother with doing it 'correctly' (which would mean by hand).\n",
    "\n",
    "As we still have a very high amount of features we drop all columns that have any nan value so that we don't have to correctly handle them. We are left with 1210 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = drop_columns_with_nan_values(df_reduced)\n",
    "display(df_reduced.shape)\n",
    "df_reduced = transform_to_categorical(df_reduced)\n",
    "display(df_reduced.shape)\n",
    "\n",
    "df_reduced.loc[:,['ti_cpi', 'ti_cpi_om']] = df_reduced.loc[:,['ti_cpi', 'ti_cpi_om']].replace(np.NaN, -5)\n",
    "df_reduced = df_reduced.dropna(how='any', axis=1).copy()\n",
    "df_reduced.loc[:,['ti_cpi', 'ti_cpi_om']] = df_reduced.loc[:,['ti_cpi', 'ti_cpi_om']].replace(-5, np.NaN)\n",
    "display(df_reduced.shape)\n",
    "df = df_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to having >1200 features we choose the best 30 according to sklearns KBest algorithm. This still leaves us with the potential problem of multicollinear features. To solve that we look at pairs of highly correlated features (>.85) and remove the one less correlated to the target variable.\n",
    "As previously mentioned we use the merged subregion to split the data into a stratified test and train set. As we use the same seed we get the same training/test split for bci and wbgi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_date_columns(df)\n",
    "\n",
    "best_features_dict = {}\n",
    "selected_features_dict = {}\n",
    "\n",
    "for target_col in corr_cols:\n",
    "    X_train, X_test, y_train, y_test = create_traintestsplit(df, corr_cols = corr_cols, meta_cols=meta_cols, target_col=target_col)\n",
    "    \n",
    "    best_features = pre_select(X_train, y_train)\n",
    "    best_features_dict[target_col] = set(best_features)\n",
    "    df_train = X_train[best_features].copy()\n",
    "    df_train[target_col]=y_train\n",
    "    mce = MultiCollinearityEliminator(df_train, target_col, 0.85)\n",
    "    feaures_no_collinearity = list(mce.autoEliminateMulticollinearity().columns)\n",
    "    feaures_no_collinearity.remove(target_col)\n",
    "    selected_features_dict[target_col] = set(feaures_no_collinearity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "To model the data we tried out a Lasso linear model and a random forest regressor. Based on our pre selection process we train our models on different feature configurations:\n",
    "\n",
    "    - the individual selected features for a particular index\n",
    "    - the intersection of all selected features for all indices\n",
    "    - the union of selected features for all indices\n",
    "    - the union of selected features for all indices without filtering based on multicollinearity\n",
    "\n",
    "To report the accuracy of the model we used the proven r2 and the rmse as metrics.\n",
    "    \n",
    "### Lasso\n",
    "The used library uses cross validation to determine a good value for alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_union=list(set.union(*list(selected_features_dict.values())))\n",
    "selected_features_intersection=list(set.intersection(*list(selected_features_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_union=list(set.union(*list(best_features_dict.values())))\n",
    "best_features_intersection=list(set.intersection(*list(best_features_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_info_script(features, name):\n",
    "    lasso_bf = dict()\n",
    "\n",
    "    df_score = pd.DataFrame(columns=['r2', 'rmse'], index=corr_cols)\n",
    "    for target in corr_cols:\n",
    "        if isinstance(features, dict):\n",
    "            lasso_bf[target] = apply_lassocv(df, target, list(features[target]), corr_cols, meta_cols, fprint=False)\n",
    "        else:\n",
    "            lasso_bf[target] = apply_lassocv(df, target, features, corr_cols, meta_cols, fprint=False)\n",
    "        df_score.loc[target,] = [lasso_bf[target]['r2'] ,lasso_bf[target]['rmse']]\n",
    "    \n",
    "    print('scores:' + name)\n",
    "    display(df_score)\n",
    "    \n",
    "lasso_info_script(selected_features_dict, ' Selected Features')\n",
    "lasso_info_script(selected_features_intersection, 'Features intersection')\n",
    "lasso_info_script(selected_features_union, 'selected Features union')\n",
    "lasso_info_script(best_features_union, ' selected Features union without collinearity filter')\n",
    "\n",
    "# print(len(best_features_union))\n",
    "# print(len(selected_features_intersection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that the models perform all very similar with a slight edge for the models with more features. This was to be expected, due to the fact, that the features where preselected. It can also be seen, that the the multicollinearity does not seem to pose a problem to the model since the unfiltered features do not lead to worse predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Next we do the same for a Random Forest Regressor. Here initially no cross validation is done. We just use a default setup at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_info_script(features, name):\n",
    "    rf_bf = dict()\n",
    "\n",
    "    df_score = pd.DataFrame(columns=['r2', 'rmse'], index=corr_cols)\n",
    "    for target in corr_cols:\n",
    "        if isinstance(features, dict):\n",
    "            rf_bf[target] = apply_rf(df, target, list(features[target]), corr_cols, meta_cols, fprint=False)\n",
    "        else:\n",
    "            rf_bf[target] = apply_rf(df, target, features, corr_cols, meta_cols, fprint=False)\n",
    "        df_score.loc[target,] = [rf_bf[target]['r2'] ,rf_bf[target]['rmse']]\n",
    "    \n",
    "    print('scores:' + name)\n",
    "    display(df_score)\n",
    "\n",
    "rf_info_script(selected_features_dict, ' Selected Features')\n",
    "rf_info_script(selected_features_intersection, 'Features intersection')\n",
    "rf_info_script(selected_features_union, 'selected Features union')\n",
    "rf_info_script(best_features_union, ' selected Features union without collinearity filter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest performs very  similarly to the lasso model allthough no parameter optimization is done yet. So we continue with Random Forrest and do hyperparameter optimization for some specific settings next to further optimize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With cross validation / hyperparameter grid search better parameters are determined. With those optimizations then again models are trained, then the test set is predicted and scores are evaluated. We where not able to set a random state for this part, to accommodate this fact we saved the results via pickle so we can work on a consistent interpretation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_gridsearch_info_script(features, name):\n",
    "    rf_bf = dict()\n",
    "\n",
    "    param_grid = {\n",
    "        \"randomforestregressor__max_depth\": [2, 3, 5, 10, None],\n",
    "        \"randomforestregressor__min_samples_split\": [2, 3, 5, 10],\n",
    "        \"randomforestregressor__max_features\": [\"log2\", None]\n",
    "        }\n",
    "\n",
    "    df_score = pd.DataFrame(columns=['r2', 'rmse'], index=corr_cols)\n",
    "    for target in corr_cols:\n",
    "        if isinstance(features, dict):\n",
    "            rf_bf[target] = apply_gridsearch_rf(df, target, list(features[target]), param_grid, corr_cols, meta_cols, fprint=False)\n",
    "        else:\n",
    "            rf_bf[target] = apply_gridsearch_rf(df, target, features, param_grid, corr_cols, meta_cols, fprint=False)\n",
    "        df_score.loc[target,] = [rf_bf[target]['r2'] ,rf_bf[target]['rmse']]\n",
    "    \n",
    "    print('score: Grit Search Random Forrest selected Features union')\n",
    "    display(df_score)\n",
    "\n",
    "    # file = os.path.join('pickle', name +'.obj')\n",
    "    # f = open(file, 'wb')\n",
    "    # pickle.dump(rf_bf ,f)\n",
    "    #f.close()    \n",
    "\n",
    "rf_gridsearch_info_script(selected_features_union, 'selected Features union')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open('pickle/rf_grid_selected_features_union.obj', 'rb')\n",
    "rf_bf = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "df_score = pd.DataFrame(columns=['r2', 'rmse'], index=corr_cols)\n",
    "for target in corr_cols:\n",
    "    df_score.loc[target,] = [rf_bf[target]['r2'] ,rf_bf[target]['rmse']]\n",
    "\n",
    "print('score: Grit Search Random Forrest selected Features union')\n",
    "display(df_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best scoring model is the 'selected Features union' one, we therefore select it for all further analysis.  In the scores table we can see, that after hyperparameter optimization the metrics of our model improved again a bit. R2 of above 0.7 has been reached for 3 out of the four corruption indices and the rmse values seem reasonable as well in regards to the scale our corruption indices operate on. \n",
    "- ti_cpi: 0-100\n",
    "- bci_bci: 0-100\n",
    "- ti_cpm: 0-10\n",
    "- wbgi_cce: -2.5-2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_fi = [rf_bf[target]['feat_importance'] for target in corr_cols]\n",
    "df_fi = pd.concat(l_fi)\n",
    "\n",
    "df_fi = df_fi.reindex(df_fi.mean().sort_values(ascending=False).index, axis=1)\n",
    "col_names = df_fi.columns\n",
    "df_fi = df_fi.T.melt(\n",
    "    ignore_index=False,\n",
    "    value_vars = ['ti_cpi', 'bci_bci', 'ti_cpi_om', 'wbgi_cce'],\n",
    "    value_name = 'feature_importance'\n",
    ").reset_index().rename(columns={'index': 'feature', 'variable': 'corruption_index'})\n",
    "\n",
    "plt.rcdefaults()\n",
    "font = {'family' : 'normal',\n",
    "    'size'   : 14}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.barplot(df_fi, x='feature',  y='feature_importance', hue='corruption_index', palette='magma', width=0.6)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.title('Feature importance')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the feature importance in the plot above we can see that the features have comparable levels of relevance to the different corruption indexes.\n",
    "The plot shows that our model sees  political and civil freedom aswell as live expectancy as good indicators for corruption levels this trend continues in the less important features aswell. This makes sense because it aligns with the general consensus that free, democratic countries suffer less from the evil that is corruption. A full list of the most important features can be seen below.\n",
    "\n",
    "| code        | Description                 |\n",
    "| ----------------- | --------------------------------------------------------------------------------------------------------------|\n",
    "| fh_pr_1.0         | Political Rights Rating (1 = most free)          |\n",
    "| fh_ipolity2       | Level of Democracy                               |\n",
    "| ihme_lifexp_0104t | Life Expectancy, Both sexes, Age 1-4 years       |\n",
    "| fh_cl_1.0         | Civil Liberties (1 = most free)                              |\n",
    "| ihme_lifexp_0104m | Life Expectancy, Male, Age 1-4 years             |\n",
    "| br_mon            | Is the country a monarchy                        |\n",
    "| cpds_vper_0.0     | Share of votes: personalist                      |\n",
    "| gd_ptss_1.0       | Political Terror Scale - US State Department (1. Countries under a secure rule of law, people are not imprisoned for their view, and torture is rare or exceptional. Political murders are extremely rare.)    |\n",
    "| kun_cluster_5.0   | Cluster memberships based on means (5 = mostly western)              |\n",
    "| wel_sys_1.0       | Political System Type (1=Unbound Autocracy)                           |\n",
    "| cpds_lall_0.0     | Share of seats in parliament: electoral alliance |\n",
    "| fh_status_1.0     | Freedom Status (1=Free)                                |\n",
    "| ciri_injud_2.0    | Independence of the Judiciary  (2=generally independent)                  |\n",
    "| fhp_status5_1.0   | Freedom of the Press, Status (2001-2016)  (1 = Free )       |\n",
    "| wel_scalezone_4.0 | Scalezone on Citizen Rights  (4=completely democratic)                    |\n",
    "| cpds_chg_0.0      | Number of changes in government per year         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_errs = get_rel_err_df(rf_bf, df, corr_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By region\n",
    "There is no big difference between performance of models across regions. ti_cpi models have slightly more problems with predicting Oceania. Oceania has smaller range or relative errors because it contains only 1 country: Papua New Guinea.\n",
    "We can also see that wbgi_cce has much larger number of outliers in comparison to other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plots_by_region(relative_errs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By sub-region\n",
    "##### Africa\n",
    "We can see that ti_cpi model performs much worse for Northern Africa than for Sub-Saharan Africa, whereas for wbgi_cce it's the opposite. The countries with the very big outliers all belong to the Sub-Saharan Africa, but they are not the same across different indices: ti_cpi and ti_cpi_om models have problems when predicting values for Somalia, but bci_bci model has problems with Botswana and wbgi_cce model has problems with predicting values for Ghana and South Africa. These problems persist over multiple years for each country, but not across indices, hence with high probability the problem lies in the prediction models and not in the mismatch between countrie's characteristics and position on the index.\n",
    "##### Europe\n",
    "Big relative error range shows that all models have more difficulties predicting values for countries from Eastern Europe. For bci_bci, the relative error for Eastern Europe is significantly worse than for other European regions. Since this trend persist over all the indeces, one could say that there is a suspicious mismatch between predictors and the estimated corruption index.\n",
    "\n",
    "For other regions different models perform slightly differently (i.e., ti_cpi and ti_cpi_om models have more problems with Southern Europe, whereas bci_bci model has difficulties with Northern Europe), but since they don't hold across the indices they are less interesting for us.\n",
    "\n",
    "Interestingly multiple models (ti_cpi_om, wgbi_cce) have Latvia from the beginning of the 2000s as an outlier, but the years are different. This could point to some bias in the corruption metrics, but would need further investigation.\n",
    "##### Asia\n",
    "Different models have slightly different performance for countries of each region of Asia, but all in all there are no major differences in the performance of the models and interesting common connections which would hold across the indices.\n",
    "##### Oceania\n",
    "As were said previously, Oceania contains only 1 country (Papua New Guinea) and only 1 sub-region (Pacific Islands) in the test set, so it is not possible to make any comparisons.\n",
    "##### Americas\n",
    "Since North America is extremely small and contains only 2 countries in total, it is not represented in the test dataset.\n",
    "\n",
    "From the big range of relative errors we can see that bci_bci model has much more difficulties predicting values for countries from Latin America and the Caribbean than countries from other sub-regions. For other models the results are comparable. The performance of other models are comparable for this region. The outlier countries are same within models, but differ across different models, so there are also no interesting connections here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plots_by_subregion(relative_errs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulties and learnings\n",
    "\n",
    "Handling a time series Dataset of a size like this brought some unique challenges that we haven't faced before. Due to the sheer size manual processing of the singular features was impossible. This meant that we had to use tools like sklearns KBest for feature reduction to get to a reasonable starting point for our models. Another big problem was to identify categorical variables since this information is not encoded into the dataset and manually adding this information with over 2000 features was not feasible. Because of this we automatically transform every variable with less then 11 unique values to one hot encoded variables. This of course leads to some ordinal variables being transformed to categorical which can also be seen in the most important features. Excluding those variables from the transformation changes the importance which leads to other features taking their place which often repeats the cycle. Even though we have over 2000 samples the number of countries per region in our test set is quite limited. This is  due to the fact that each sample belongs to a country and a year, and we have to split test and train sets in a way that all data points of one country are either in the test or the train set. We do this to ensure that our test data is unseen because the differences in between years might be too small. Besides the difficulties we improved our knowledge of ```pandas```, ```seaborn```, ```sklearn``` and also learned completly new libraries like ```plotly``` and ```msno```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biases\n",
    "\n",
    "One bias might be the small sample size of certain regions, which leads to under-representation in the model and therefore higher uncertainty which could lead to an inaccurate prediction. But the bigger issues stem from the data itself. All our target values and most features are collected by western institutions which could lead to some biases due to imposing western values and standards onto the non western part of the world. Apart from that western institutions might unintentionally tend to western favoring metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work split\n",
    "\n",
    "Every teammate was involved in every step, but there were still some main focus points:\n",
    "\n",
    "Godun Alina: Feature Selection, Evaluation\n",
    "Habeger David: Data Exploration, Data preprocessing\n",
    "Haberl Alexander: Data Exploration, Data preprocessing\n",
    "Konzett Siemon: Modeling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dopp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct 24 2022, 16:07:47) [GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "543aa23b88fc9de87e855576d5d47491130665cd749e56028ebe7cfa2e718f2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
